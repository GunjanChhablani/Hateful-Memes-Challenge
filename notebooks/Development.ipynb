{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import jsonlines\n",
    "import yaml\n",
    "\n",
    "import os\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark= False\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigMapper:\n",
    "    \"\"\"Class for creating ConfigMapper objects.\n",
    "\n",
    "    This class can be used to create custom configuration names using YAML files.\n",
    "    For each class or object instantiated in any modules,\n",
    "    the ConfigMapper object can be used either with the functions,\n",
    "    or as a decorator to store the mapping in the function.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    \n",
    "    \"\"\"\n",
    "    dicts = {\n",
    "        \"models\":{},\n",
    "        \"trainers\":{},\n",
    "        \"metrics\":{},\n",
    "        \"losses\":{},\n",
    "        \"optimizers\":{},\n",
    "        \"schedulers\":{},\n",
    "        \"devices\":{},\n",
    "        \"transforms\":{},\n",
    "        \"params\":{}\n",
    "    }\n",
    "    @classmethod\n",
    "    def map(cls,key,name):\n",
    "        \"\"\"\n",
    "        Map a particular name to an object, in the specified key\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            name : str\n",
    "                The name of the object which will be used.\n",
    "            key : str\n",
    "                The key of the mapper to be used.\n",
    "        \"\"\"\n",
    "        def wrap(obj):\n",
    "            if(key in cls.dicts.keys()):\n",
    "                cls.dicts[key][name]=obj\n",
    "            else:\n",
    "                cls.dicts[key] = {}\n",
    "                cls.dicts[key][name]=obj\n",
    "            return obj\n",
    "        return wrap\n",
    "\n",
    "    @classmethod\n",
    "    def get_object(cls,key,name):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return cls.dicts[key][name]\n",
    "        except:\n",
    "            raise NotImplementedError('Key Undefined.')\n",
    "configmapper = ConfigMapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    \"\"\"\n",
    "    Function to load a yaml file and\n",
    "    return the collected dict(s)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path to the yaml config file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : dict\n",
    "        The dictionary from the config file\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(path,str), \"Provided path is not a string\"\n",
    "    try:\n",
    "        f = open(path,'r')\n",
    "        result = yaml.load(f,Loader=yaml.Loader)\n",
    "    except FileNotFoundError as e:\n",
    "        # Adding this for future functionality\n",
    "        raise e\n",
    "    return result\n",
    "class Config:\n",
    "    \"\"\"Config Class to be used with YAML configuration files\n",
    "\n",
    "    This class can be used to address keys as attributes.\n",
    "    Ensure that there are no spaces between the keys.\n",
    "    Only objects of type dict can be converted to config.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _config : dict,\n",
    "        The dictionary which is formed from the\n",
    "        yaml file or custom dictionary\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    as_dict(),\n",
    "        Return the config object as dictionary\n",
    "\n",
    "        Possible update:\n",
    "        ## Can be converted using __getattr__ to use **kwargs\n",
    "        ## with the Config object directly.\n",
    "\n",
    "    set_value(attr,value)\n",
    "        Set the value of a particular attribute.\n",
    "    \"\"\"\n",
    "    def __init__(self,*,path=None,dic=None):\n",
    "        \"\"\"\n",
    "        Initializer for the Config class\n",
    "\n",
    "        Needs either path or the dict object to create the config\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str, optional\n",
    "            The path to the config YAML file.\n",
    "            Default value is None.\n",
    "        dic : dict, optional\n",
    "            The dictionary containing the configuration.\n",
    "            Default value is None.\n",
    "        \"\"\"\n",
    "        if(path):\n",
    "            self._config = load_yaml(path)\n",
    "        elif(dict):\n",
    "            self._config = dic\n",
    "        else:\n",
    "            raise Exception('Need either path or dict object to instantiate object.')\n",
    "        # self.keys = self._config.keys()\n",
    "\n",
    "    def __getattr__(self,attr):\n",
    "        \"\"\"\n",
    "        Get method for Config class. Helps get keys as attributes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        attr: The attribute name passed as <object>.attr\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self._config[attr]: object or Config object\n",
    "            The value of the given key if it exists.\n",
    "            If the value is a dict object,\n",
    "            a Config object of that dict is returned.\n",
    "            Otherwise, the exact value is returned.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        KeyError() if the given key is not defined.\n",
    "        \"\"\"\n",
    "        if(attr in self._config):\n",
    "            if(isinstance(self._config[attr],dict)):\n",
    "                return Config(dic=self._config[attr])\n",
    "            else:\n",
    "                return self._config[attr]\n",
    "        else:\n",
    "            raise KeyError(f\"Key:{attr} not defined.\")\n",
    "    def set_value(self,attr,value):\n",
    "        \"\"\"\n",
    "        Set method for Config class. Helps set keys in the _config.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        attr: The attribute name passed as <object>.attr\n",
    "        value: The value to be stored as the attr.\n",
    "        \"\"\"\n",
    "        self._config[attr]=value\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\" Function to print the dictionary\n",
    "         contained in the object.\"\"\"\n",
    "        return self._config.__str__()\n",
    "\n",
    "    def as_dict(self):\n",
    "        \"\"\"Function to get the config as dictionary object\"\"\"\n",
    "        return dict(self._config)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_params_to_dict(params):\n",
    "    dic = {}\n",
    "    for k,v in params.as_dict():\n",
    "        try:\n",
    "            obj = configmapper.get_object('params',v)\n",
    "            dic[k]=v\n",
    "        except:\n",
    "            print(f\"Undefined {v} for the given key: {k} in mapper        ,storing original value\")\n",
    "            dic[k]=v\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_config = Config(path = '../configs/default.yaml')\n",
    "model_config = Config(path = '../configs/models/unimodal/image.yaml')\n",
    "trainer_config = Config(path = '../configs/trainer.yaml')\n",
    "data_config = Config(path='../configs/data.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonlReader:\n",
    "    \"\"\"A class for reading jsonl files easily\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    _path : str\n",
    "        The file location of the jsonl file\n",
    "    _reader: jsonlines.Reader object\n",
    "        The object which reads the files and keeps a tracker on the file\n",
    "    _size : int\n",
    "        Number of lines to be read\n",
    "    _read : int\n",
    "        Number of lines read so far\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    size()\n",
    "        Getter method for _size.\n",
    "    path()\n",
    "        Getter method for _path.\n",
    "    resetReader()\n",
    "        Resets the _reader, starts over again.\n",
    "    readNext(loop=False)\n",
    "        Read the next line from the file. Loop over if loop=True.\n",
    "    read(count=None, loop=False)\n",
    "        Read the next count lines from the file, until self._size lines are read in total. Loop over if loop=True.\n",
    "    close()\n",
    "        Closes the reader of the file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,path,size=None):\n",
    "        \"\"\"\n",
    "        Initializes the JsonReader object.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        path : string\n",
    "            The path to the .jsonl file.\n",
    "        size : int, optional\n",
    "            The number of lines to be read from the file.\n",
    "            If None, size is set to total number of lines.\n",
    "            (default is None).\n",
    "        \"\"\"\n",
    "\n",
    "        self._path = path\n",
    "        self._reader = jsonlines.open(path,'r')\n",
    "        if(size):\n",
    "            self._size = size\n",
    "        else:\n",
    "            self._size = len(list(iter(jsonlines.open(path,'r'))))\n",
    "        self._read = 0\n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"\n",
    "        Function to get the _size attribute.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            self._size : int\n",
    "                The number of lines which will be read from the file\n",
    "        \"\"\"\n",
    "        return self._size\n",
    "\n",
    "    @property\n",
    "    def path(self):\n",
    "        \"\"\"\n",
    "        Function to get the _path attribute.\n",
    "                Returns\n",
    "        -------\n",
    "            self._path : str\n",
    "                The path of the the file\n",
    "        \"\"\"\n",
    "\n",
    "        return self._path\n",
    "\n",
    "    def resetReader(self):\n",
    "        \"\"\"Function to reset the reader attribute\n",
    "\n",
    "        This function starts reading the file from the beginning.\n",
    "        \"\"\"\n",
    "\n",
    "        self._reader = jsonlines.open(self._path,'r')\n",
    "        self._read = 0\n",
    "    def readNext(self,loop=False):\n",
    "        \"\"\"\n",
    "        Function to read the next line from the path.\n",
    "\n",
    "        If the loop parameter is set to True,\n",
    "        then the reader starts over if the file ends.\n",
    "\n",
    "        Raises an Exception if the file ends,\n",
    "        and if the loop parameter is False.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            loop : bool, optional\n",
    "               Signifies whether the reader should start reading again\n",
    "               if the end of file is reached. (default is False)\n",
    "        Returns\n",
    "        -------\n",
    "            result: dict\n",
    "                Returns next line of the file as a dict.\n",
    "        \"\"\"\n",
    "        if(self._read>=self._size):\n",
    "            print(\"Max count reached. Reset to start again.\")\n",
    "            return\n",
    "        try:\n",
    "            result = self._reader.read()\n",
    "            self._read+=1\n",
    "        except EOFError as e:\n",
    "            if(loop==False):\n",
    "                print('End of File reached.')\n",
    "                return\n",
    "            else:\n",
    "                self.resetReader()\n",
    "                try:\n",
    "                    result =  self._reader.read()\n",
    "                    self._read+=1\n",
    "                except:\n",
    "                    print('Empty File. Aborting Read')\n",
    "                    return\n",
    "        return result\n",
    "    def read(self,count=None,loop=False):\n",
    "        \"\"\"\n",
    "        Function to read the next count lines from the file.\n",
    "\n",
    "        If the loop parameter is set to True,\n",
    "        then the reader starts over if the file ends.\n",
    "\n",
    "        Raises an Exception if the file ends,\n",
    "        and returns the collected lines.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            count : int, optional\n",
    "                The number of lines to be read. If None,\n",
    "                reads all the remaining lines. (default is None)\n",
    "\n",
    "            loop : bool, optional\n",
    "               Signifies whether the reader should start reading again\n",
    "               if the end of file is reached. (default is False).\n",
    "               Insignificant if the count is None.\n",
    "        Returns\n",
    "        -------\n",
    "            lines: list of dict\n",
    "                Returns the lines collected from the file.\n",
    "        \"\"\"\n",
    "\n",
    "        lines = []\n",
    "        if(count is None):\n",
    "            for line in self._reader:\n",
    "                self._read+=1\n",
    "                if(self._read<=self._size):\n",
    "                    lines.append(line)\n",
    "                else:\n",
    "                    break\n",
    "            return lines\n",
    "        for line in range(count):\n",
    "            try:\n",
    "                result = self.readNext(loop=loop)\n",
    "                if(result):\n",
    "                    lines.append(result)\n",
    "                else:\n",
    "                    raise Exception('None received at self.readNext')\n",
    "            except:\n",
    "                print('Error occurred. Returning collected lines so far.')\n",
    "        return lines\n",
    "    def close(self):\n",
    "        \"\"\" Function to close the reader \"\"\"\n",
    "        self._reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,model,trainer,log_dir,comment=None):\n",
    "        \"\"\" Initializer for Logger Class\n",
    "        #Arguments:\n",
    "\n",
    "        \"\"\"\n",
    "        self.model_path = os.path.join(log_dir,model,trainer)\n",
    "        self.writer = SummaryWriter(log_dir=self.model_path,comment=comment)\n",
    "        try:\n",
    "            if(not os.exists(log_dir)):\n",
    "                os.makedir(log_dir)\n",
    "            if(not(os.exists(self.model_path))):\n",
    "                os.makedir(self.model_path)\n",
    "            else:\n",
    "                print(\"Directory Already Exists.\")\n",
    "        except:\n",
    "            print(\"Failed to Create Directory.\")\n",
    "\n",
    "\n",
    "    def save_params(self,param_list,param_name_list,epoch,batch_size,batch=None,combine=False,combine_name=None):\n",
    "        if(combine==False):\n",
    "            for i in range(len(param_list)):\n",
    "                if(isinstance(param_list[i],Variable)):\n",
    "                    param_list[i] = param_list[i].data.cpu().numpy()\n",
    "                    self.writer.add_scalar(param_name_list[i],param_list[i],Logger._global_step(epoch,batch_size,batch))\n",
    "\n",
    "        else:\n",
    "            scalar_dict = dict(zip(param_name_list,param_list))\n",
    "            self.writer.add_scalars(combine_name,scalar_dict,Logger._global_step(epoch,batch_size,batch))\n",
    "\n",
    "\n",
    "    def save_batch_images(self,image_name,image_batch,epoch,batch_size,batch=None,dataformats = 'CHW'):\n",
    "        self.writer.add_images(image_name,image_batch,Logger._global_step(epoch,batch_size,batch),dataformats=dataformats)\n",
    "\n",
    "    def save_prcurve(self,labels,preds,epoch,batch_size,batch=None):\n",
    "        self.writer.add_pr_curve('pr_curve',labels,preds,Logger._global_step(epoch,batch_size,batch))\n",
    "\n",
    "    def save_hyperparams(self,hparam_list,hparam_name_list,metric_list,metric_name_list):\n",
    "        self.writer.add_hparams(zip(metric_name_list,metric_list),zip(hparam_name_list,hparam_list))\n",
    "\n",
    "    def save_models(self,model_list,model_names_list,epoch):\n",
    "        for model_name,model in zip(model_names_list,model_list):\n",
    "            torch.save(model.state_dict(),os.path.join(self.model_path,model_name))\n",
    "\n",
    "    def save_fig(self,fig,fig_name,epoch,batch_size,batch=None):\n",
    "        self.writer.add_figure(fig_name,fig,Logger._global_step(epoch,batch_size,batch))\n",
    "\n",
    "    def display_params(params_list,params_name_list,epoch,num_epochs,batch_size,batch):\n",
    "        for i in range(len(params_list)):\n",
    "            if isinstance(params_list[i],Variable):\n",
    "                params_list[i] = params_list[i].data.cpu().numpy()\n",
    "        print('Epoch: {}/{}, Batch: {}/{}'.format(epoch,num_epochs,batch,batch_size))\n",
    "        for i in range(len(params_list)):\n",
    "            print('{}:{}'.format(params_name_list[i],params_list[i]))\n",
    "\n",
    "    def draw_model_architecture(model,output,input,input_name,save_name):\n",
    "        make_dot(output,params = dict(list(model.named_parameters()))+[(input_name,input)])\n",
    "\n",
    "    def __del__(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def _global_step(epoch,batch_size,batch):\n",
    "        if(batch):\n",
    "            return epoch*batch_size + batch\n",
    "        else:\n",
    "            return epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dict_to_obj(dic):\n",
    "    result_dic = {}\n",
    "    if(dic is not None):\n",
    "        for k,v in dic.items():\n",
    "            if(isinstance(v,dict)):\n",
    "                result_dic[k]=map_dict_to_obj(v)\n",
    "            else:\n",
    "                try:\n",
    "                    obj = configmapper.get_object('params',v)\n",
    "                    result_dic[k]=obj\n",
    "                except:\n",
    "                    result_dic[k]=v\n",
    "    return result_dic\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "def get_image_processor(processor):\n",
    "    transformations = []\n",
    "    if(processor.type=='torchvision'):\n",
    "        for param in processor.params:\n",
    "            transformations.append(configmapper.get_object('transforms',param['type'])(**map_dict_to_obj(param['params'])))\n",
    "    return transforms.Compose(transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class MemesDataset(Dataset):\n",
    "  \"\"\" Dataset class for Hateful Memes \"\"\"\n",
    "  def __init__(self,config,typ='train'):\n",
    "    \"\"\"Init Function for the Dataset Class\n",
    "\n",
    "    Parameters:\n",
    "    ------\n",
    "    config : The Config object containing configuration for data\n",
    "    typ : String value specifying whether it is train,dev or test\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    self._config = config\n",
    "    self.type = typ\n",
    "    self.reader = JsonlReader(self._config.annotations.as_dict()[typ])\n",
    "    self.annotations = self.reader.read()\n",
    "    self.transform = get_image_processor(self._config.image_processor)\n",
    "    \n",
    "  def __len__(self):\n",
    "    \"\"\"Function to return the size of the data\"\"\"\n",
    "    return self.reader.size\n",
    "  def __getitem__(self,idx):\n",
    "    record_dic=self.annotations[idx]\n",
    "    img = Image.open(os.path.join(self._config.data_dir,record_dic['img'])).convert('RGB')\n",
    "    text = record_dic['text']\n",
    "    if self.transform:\n",
    "        img = self.transform(img)\n",
    "    label =record_dic['label']\n",
    "    if(self.type in ['train','dev']):\n",
    "      if(self._config.get_image):\n",
    "        if(self._config.get_text):\n",
    "          return img,text,label\n",
    "        else:\n",
    "          return img,label\n",
    "      elif(self._config.get_text):\n",
    "        return text,label\n",
    "      else:\n",
    "        raise Exception('Need atleast some features to return.')\n",
    "        return\n",
    "    else:\n",
    "      if(self._config.get_image):\n",
    "        if(self._config.get_text):\n",
    "          return img,text\n",
    "        else:\n",
    "          return img\n",
    "      elif(self._config.get_text):\n",
    "        return text\n",
    "      else:\n",
    "        raise Exception('Need atleast some features')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,batch in enumerate(DataLoader(dataset,batch_size=12)):\n",
    "#     print(batch[0],batch[1])\n",
    "#     if(i>2):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@configmapper.map(\"models\",\"unimodal\")\n",
    "class Unimodal(nn.Module):\n",
    "  def __init__(self,config):\n",
    "    super(Unimodal,self).__init__()\n",
    "    self._config = config\n",
    "    self.mode = config.mode\n",
    "    if(self.mode == 'image'):\n",
    "        self.modal_encoder,in_features = get_backbone(config.modal_encoder)\n",
    "        self.classifier = get_classifier(config.classifier)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "  def forward(self,x):\n",
    "    if(self.mode=='image'):\n",
    "        x = self.flatten(self.modal_encoder(x))\n",
    "        x = self.classifier(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(modal_encoder):\n",
    "    if(modal_encoder.type=='resnet152'):\n",
    "        model = torch.hub.load('pytorch/vision:v0.6.0','resnet152',pretrained=modal_encoder.params.pretrained)\n",
    "    in_features = model.fc.in_features\n",
    "    if(modal_encoder.params.remove_classifier):\n",
    "      model = nn.Sequential(*list(model.children())[:-1])\n",
    "    return model,in_features\n",
    "def get_classifier(classifier):\n",
    "    layers =[]\n",
    "    if(classifier.custom_layers is None):\n",
    "        if(classifier.type =='mlp'):\n",
    "            for layer in range(classifier.params.num_layers):\n",
    "                if(layer==0):\n",
    "                    layers.append(nn.Linear(classifier.params.in_dim,classifier.params.hidden_dims[0]))\n",
    "                    layers.append(configmapper.get_object('activations',classifier.params.activation.default.name)(**classifier.params.activation.default.params.as_dict()))\n",
    "                    #print(layers)\n",
    "                elif(layer==classifier.params.num_layers-1):\n",
    "                    layers.append(nn.Linear(classifier.params.hidden_dims[-1],classifier.params.out_dim))\n",
    "                    layers.append(configmapper.get_object('activations',classifier.params.activation.output.name)(**classifier.params.activation.output.params.as_dict()))\n",
    "                    #print(layers)\n",
    "                else:\n",
    "                    layers.append(nn.Linear(classifier.params.hidden_dims[layer],classifier.params.hidden_dims[layer+1]))\n",
    "                    layers.append(configmapper.get_object('activations',classifier.params.activation.default.name)(**classifier.params.activation.default.params.as_dict()))\n",
    "    #print(layers)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.loss.CrossEntropyLoss"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "configmapper.map('losses','cross_entropy')(CrossEntropyLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from torchnlp.metrics import get_accuracy\n",
    "\n",
    "@configmapper.map('metrics','binary_auroc')\n",
    "def binary_auroc(outputs,labels):\n",
    "    \"\"\"Function to compute Area Under ROC Curve Score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs: torch.Tensor\n",
    "        Tensor containing the softmax outputs from the model\n",
    "    labels: torch.Tensor\n",
    "        Tensor containing the labels (Not one-hot encoded)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roc_auc_score : int,\n",
    "        The roc_auc_score computed between the outputs and the labels\n",
    "\n",
    "    ## Update tips:\n",
    "    ## More functionality can be added through the parameters.\n",
    "    ## Custom AUROC function/class can also be defined.\n",
    "    \"\"\"\n",
    "    outputs_index = outputs[:,1]\n",
    "    return roc_auc_score(labels.detach().numpy(),outputs_index.detach().numpy())\n",
    "\n",
    "@configmapper.map('metrics','accuracy')\n",
    "def accuracy(outputs,labels):\n",
    "    \"\"\"Function to compute Accuracy Score\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs: torch.Tensor\n",
    "        Tensor containing the softmax outputs from the model\n",
    "    labels: torch.Tensor\n",
    "        Tensor containing the labels (Not one-hot encoded)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy_score : int\n",
    "        The accuracy_score computed between the outputs and the labels\n",
    "\n",
    "    ## Update tips:\n",
    "    ## More functionality can be added through the parameters.\n",
    "    ## Custom function/class can also be defined.\n",
    "    \"\"\"\n",
    "    outputs_argmax = torch.argmax(outputs,dim=1)\n",
    "    return get_accuracy(labels,outputs_argmax)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.adamw.AdamW"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "configmapper.map('optimizers','adam_w')(AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "configmapper.map('transforms','Resize')(transforms.Resize)\n",
    "configmapper.map('transforms','Normalize')(transforms.Normalize)\n",
    "configmapper.map('transforms','ToTensor')(transforms.ToTensor)\n",
    "configmapper.map('params','BICUBIC')(Image.BICUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "configmapper.map('schedulers','cosine_warm')(CosineAnnealingWarmRestarts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.LogSoftmax"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "configmapper.map('activations','relu')(nn.ReLU)\n",
    "configmapper.map('activations','logsoftmax')(nn.LogSoftmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "@configmapper.map(\"trainers\",\"trainer\")\n",
    "class Trainer:\n",
    "    def __init__(self,config):\n",
    "        self._config = config\n",
    "        self.metrics = [configmapper.get_object('metrics',metric) for metric in self._config.main_config.metrics]\n",
    "        self.train_config = self._config.train\n",
    "        self.eval_config = self._config.eval\n",
    "## Train\n",
    "    def train(self,model,dataset,verbose,tqdm_out=True,eval_dataset=None):\n",
    "\n",
    "        optim_params = self.train_config.optimizer.params\n",
    "        if(optim_params):\n",
    "            optimizer = configmapper.get_object('optimizers',self.train_config.optimizer.type)(model.parameters(),**map_dict_to_obj(optim_params.as_dict()))\n",
    "        else:\n",
    "            optimizer = configmapper.get_object('optimizers',self.train_config.optimizer.type)(model.parameters())\n",
    "        \n",
    "        scheduler_params = self.train_config.scheduler.params\n",
    "        if(scheduler_params):\n",
    "            scheduler = configmapper.get_object('schedulers',self.train_config.scheduler.type)(optimizer,**map_dict_to_obj(scheduler_params.as_dict()))\n",
    "        else:\n",
    "            scheduler = configmapper.get_object('schedulers',self.train_config.scheduler.type)(optimizer)\n",
    "            \n",
    "        criterion_params = self.train_config.criterion.params\n",
    "        if(criterion_params):\n",
    "            criterion = configmapper.get_object('losses',self.train_config.criterion.type)(**map_dict_to_obj(criterion_params.as_dict()))\n",
    "        else:\n",
    "            criterion = configmapper.get_object('losses',self.train_config.criterion.type)()\n",
    "            \n",
    "        train_loader = DataLoader(dataset,**self.train_config.loader_params.as_dict())\n",
    "        train_logger = Logger(**self.train_config.log.logger_params.as_dict())\n",
    "        log_interval = self.train_config.log.log_interval\n",
    "        eval_interval = self.train_config.eval_interval\n",
    "        max_steps = self.train_config.max_steps\n",
    "        log_values = self.train_config.log.values.as_dict()\n",
    "\n",
    "        step=0\n",
    "        epoch=0\n",
    "        break_all=False\n",
    "        if(tqdm_out):\n",
    "            pbar = tqdm(total = max_steps)\n",
    "        print('Starting training...')\n",
    "        print(max_steps)\n",
    "        while(step<max_steps):\n",
    "            epoch+=1\n",
    "            running_loss = 0\n",
    "            all_labels = torch.LongTensor()\n",
    "            all_outputs = torch.Tensor()\n",
    "            for i,batch in enumerate(train_loader):\n",
    "                if(step>=max_steps):\n",
    "                    break_all = True\n",
    "                    break\n",
    "                step+=1\n",
    "                *inputs, labels = [value.to(torch.device(self._config.main_config.device.name)) for value in batch]\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs,labels)\n",
    "                loss.backward()\n",
    "                all_labels = torch.cat((all_labels,labels),0)\n",
    "                all_outputs = torch.cat((all_outputs,outputs),0)\n",
    "                running_loss+=loss.item()\n",
    "                optimizer.step()\n",
    "                scheduler.step(epoch + i/len(train_loader))\n",
    "                \n",
    "                \n",
    "                if(step%log_interval==log_interval-1):\n",
    "                    if(log_values['loss']):\n",
    "                        train_logger.save_params([loss.item()/self.train_config.loader_params.batch_size],['train_loss'],epoch=epoch,batch_size=self.train_config.loader_params.batch_size,batch=i+1)\n",
    "\n",
    "                    if(log_values['metrics']):\n",
    "                        train_logger.save_params([metric(outputs,labels) for metric in self.metrics],[metric for metric in self._config.main_config.metrics],combine=True,combine_name='metrics',epoch=epoch,batch_size=self.train_config.loader_params.batch_size,batch=i+1)\n",
    "\n",
    "                if(eval_dataset is not None and step%eval_interval==eval_interval-1):\n",
    "                    self.eval(model,eval_dataset,epoch,i,log_values,criterion)\n",
    "                pbar.update(1)\n",
    "                \n",
    "    \n",
    "            loss = running_loss/len(train_loader)\n",
    "            loss_list = [loss]\n",
    "            loss_name_list = ['train_loss']\n",
    "            \n",
    "            if(log_values['loss']):\n",
    "                train_logger.save_params(loss_list,loss_name_list,epoch=epoch,batch_size=self.train_config.loader_params.batch_size,batch=self.train_config.loader_params.batch_size)\n",
    "\n",
    "            metric_list =[metric(all_outputs,all_labels) for metric in self.metrics]\n",
    "            metric_name_list= [metric for metric in self._config.main_config.metrics]\n",
    "            if(log_values['metrics']):\n",
    "                train_logger.save_params(metric_list,metric_name_list,combine=True,combine_name='metrics',epoch=epoch,batch_size=self.train_config.loader_params.batch_size,batch=self.train_config.loader_params.batch_size)\n",
    "            print(f'Train, Epoch:{epoch}')\n",
    "            print(loss_list+metric_list)\n",
    "            print(loss_name_list+metric_name_list)\n",
    "\n",
    "            if(break_all):\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        if(tqdm_out):\n",
    "            pbar.close()\n",
    "\n",
    "## Evaluate\n",
    "    def eval(self,model,dataset,epoch,i,log_values,criterion):\n",
    "        eval_logger = Logger(**self.eval_config.log.logger_params.as_dict())\n",
    "        eval_loader = DataLoader(dataset,**self.eval_config.loader_params.as_dict())\n",
    "        all_outputs = torch.Tensor()\n",
    "        all_labels = torch.LongTensor()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for j,batch in enumerate(eval_loader):\n",
    "                *inputs, labels = [value.to(torch.device(self._config.main_config.device.name)) for value in batch]\n",
    "                outputs = model(*inputs)\n",
    "                loss = criterion(outputs,labels)\n",
    "                val_loss+=loss.item()\n",
    "                all_labels = torch.cat((all_labels,labels),0)\n",
    "                all_outputs = torch.cat((all_outputs,outputs),0)\n",
    "            val_loss = val_loss/len(eval_loader)\n",
    "            loss_list = [val_loss]\n",
    "            loss_name_list = ['eval_loss']\n",
    "            if(log_values['loss']):\n",
    "                val_logger.save_params(loss_list,loss_name_list,epoch=epoch,batch_size=self.train_config.loader_params.batch_size,batch=i+1)\n",
    "\n",
    "            metric_list =[metric(all_outputs,all_labels) for metric in self.metrics]\n",
    "            metric_name_list= [metric for metric in self._config.main_config.metrics]\n",
    "            if(log_values['metrics']):\n",
    "                val_logger.save_params(metric_list,metric_name_list,combine=True,combine_name='metrics',epoch=epoch,batch_size=self.eval_config.loader_params.batch_size,batch=i+1)\n",
    "            print('Evaluation:')\n",
    "            print(loss_list+metric_list)\n",
    "            print(loss_name_list+metric_name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/crocoder/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "model = Unimodal(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MemesDataset(data_config,'train')\n",
    "dev = MemesDataset(data_config,'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to Create Directory.\n",
      "Starting training...\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:17<00:17, 17.94s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 2/2 [00:31<00:00, 15.71s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Epoch:1\n",
      "[0.046265465920667555, 0.3473684210526316, 0.5833333333333334]\n",
      "['train_loss', 'binary_auroc', 'accuracy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model,dataset,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'trainer', 'version': 1.0, 'main_config': {'seed': 42, 'metrics': ['binary_auroc', 'accuracy'], 'device': {'name': 'cpu'}}, 'train': {'seed': 42, 'metrics': ['binary_auroc', 'accuracy'], 'device': {'name': 'cpu'}, 'max_steps': 2, 'eval_interval': 1, 'loader_params': {'batch_size': 12, 'num_workers': 4, 'shuffle': True}, 'optimizer': {'type': 'adam_w', 'params': {'lr': 0.01, 'betas': [0.9, 0.998], 'eps': 1e-08}}, 'scheduler': {'type': 'cosine_warm', 'params': {'T_0': 1, 'T_mult': 2, 'eta_min': 1e-07}}, 'criterion': {'type': 'cross_entropy', 'params': None}, 'log': {'log_interval': 1, 'logger_params': {'model': 'unimodal', 'trainer': 'trainer', 'comment': 'Trial for Logger and Trainer', 'log_dir': './logs/train'}, 'values': {'loss': True, 'metrics': True}}}, 'eval': {'seed': 42, 'metrics': ['binary_auroc', 'accuracy'], 'device': {'name': 'cpu'}, 'loader_params': {'batch_size': 12, 'num_workers': 4, 'shuffle': False}, 'log': {'logger_params': {'model': 'unimodal', 'trainer': 'trainer', 'comment': 'Trial for Logger and Trainer', 'log_dir': './logs/eval'}, 'values': {'metrics': True}}}}\n"
     ]
    }
   ],
   "source": [
    "print(trainer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
